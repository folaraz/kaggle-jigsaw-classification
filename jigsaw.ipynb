{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, Dense, CuDNNGRU,concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50000)\n",
    "pd.set_option('display.width', 50000)\n",
    "tqdm.pandas()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Practise is British. Practice is used in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1804874, 3)\n",
      "Test shape :  (97320, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text\n",
       "0  59848     0.0  This is so cool. It's like, 'would you want yo...\n",
       "1  59849     0.0  Thank you!! This would make my life a lot less..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', usecols=['id','comment_text', 'target'])\n",
    "test = pd.read_csv('test.csv')\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embeddings_index = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_trump(word):\n",
    "    if word =='tRump':\n",
    "        return 'trump'\n",
    "    else: \n",
    "        return word\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences,disable = (not True)):\n",
    "        for word in sentence:\n",
    "            if word in ['and','of','to', 'a']:\n",
    "                pass\n",
    "            else:\n",
    "                word = clean_website_link(word)\n",
    "                word = clean_trump(word)\n",
    "                try:\n",
    "                    vocab[word] += 1\n",
    "                except KeyError:\n",
    "                    vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "import operator \n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '=':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '@':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<>[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x \n",
    "\n",
    "\n",
    "def clean_website_link(site):\n",
    "    if 'www' in site[:4]:\n",
    "        return site[:-3][3:]\n",
    "    else:\n",
    "        return site\n",
    "    \n",
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'centres':'centers',\n",
    "                'cheque':'check',\n",
    "                'cheques':'checks',\n",
    "                'didnt':'did not',\n",
    "                'offence':'offense',\n",
    "                'offences':'offenses',\n",
    "                'favour':'favor',\n",
    "                'favours':'favors',\n",
    "                'realise':'realize',\n",
    "                'doesnt':'does not',\n",
    "                'defence':'defense',\n",
    "                'isnt':'is not',\n",
    "                'humour':'humor',\n",
    "                'theatre':'theater',\n",
    "                'grey':'gray',\n",
    "                'neighbour':'neighbor',\n",
    "                'neighbours':'neighbors',\n",
    "                'neighbourhood':'neighborhood',\n",
    "                'neighbourhoods':'neighborhoods',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'honour':'honor',\n",
    "                'selfies':'pictures',\n",
    "                'selfie':'picture',\n",
    "                'cancelled':'canceled',\n",
    "                'labelled':'labeled',\n",
    "                'labour':'labor',\n",
    "                'licence':'license',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                'signalling':'signaling',\n",
    "                'travelling':'traveling',\n",
    "                'judgements':'judgments',\n",
    "                'cancelling':'canceling',\n",
    "                'favoured':'favored',\n",
    "                'colours':'colors' ,\n",
    "                'organisation':'organization',\n",
    "                'litre':'liter',\n",
    "                'acknowledgement':'acknowledgment',\n",
    "                'judgement':'judgment',\n",
    "                'honourable':'honorable',\n",
    "                'Koncerned':'concerned',\n",
    "                'dubia':'Dubia',\n",
    "                'analyses':'analysis',\n",
    "                'travelled':'traveled',\n",
    "                '…and':'and',\n",
    "                'twittercom':'twitter',\n",
    "                'Selfie':'picture',\n",
    "                'favourable':'favorable',\n",
    "                'recognise':'recognize',\n",
    "                'behaviours':'behaviors',\n",
    "                'behaviour':'behavior',\n",
    "                'OBAMAcare':'obamacare',\n",
    "                'coloured':'colored',\n",
    "                'counselling':'counseling',\n",
    "                'honoured':'honored',\n",
    "                'coloured':'colored',\n",
    "                'googl':'google',\n",
    "                'Humanae':'Humane',\n",
    "                'worshippers':'worshipers',\n",
    "                'kilometres':'kilometers',\n",
    "                'kilometre':'kilometer',\n",
    "                'metre':'meter',\n",
    "                'metres':'meters',\n",
    "                'worshipper':'worshiper',\n",
    "                'telegraphc':'telegraphic',\n",
    "                'endeavour':'endeavor',\n",
    "                'endeavours':'endeavors',\n",
    "                'labelling':'labeling',\n",
    "                'arse':'ass',\n",
    "                'programme':'programe',\n",
    "                'judgemental':'judgmental',\n",
    "                '…you':'you',\n",
    "                'CONservative':'conservative',\n",
    "                'travellers':'travelers',\n",
    "                '…the': 'the',\n",
    "                'canadas':'canada',\n",
    "                'TheDonald':'Donald',\n",
    "                'tRUMP':'trump',\n",
    "                'practise':'practice',\n",
    "                'criticise':'criticize',\n",
    "                'criticises':'criticizes',\n",
    "                'flavour':'flavor',\n",
    "                'flavours':'flavors',\n",
    "                'Deplorables':'Deplorable',\n",
    "                'worshipping': 'worshiping',\n",
    "                'apologise':'apologize',\n",
    "                'learnt':'learned',\n",
    "                'harbour':'harbor',\n",
    "                '…that':'that',\n",
    "                'fuelled':'fueled',\n",
    "                'rumours':'rumors',\n",
    "                'rumour':'rumor',\n",
    "                'programmes':'programes',\n",
    "                'realDonaldTrump':'Trump',\n",
    "                'saviour':'savior',\n",
    "                'saviours':'saviors',\n",
    "                'trads':'trad',\n",
    "                'organised':'organized',\n",
    "                'dependant':'dependent',\n",
    "                'armoured':'armored',\n",
    "                'fibre':'fiber',\n",
    "                '…it':'it',\n",
    "                'enquiry':'inquiry',\n",
    "                'enquiries':'inquiries',\n",
    "                'wilfully':'willfully',\n",
    "                'wilfull':'willfull',\n",
    "                'councillors':'councilors',\n",
    "                'subsidised':'subsidized'\n",
    "\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')     \n",
    "    return text\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')     \n",
    "    return text\n",
    "\n",
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punct_mapping = {}\n",
    "for i in unknown_punct(embeddings_index, punct):\n",
    "    punct_mapping[i] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "                       \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
    "                       \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   Google News :\n",
      "[\"ain't\", \"aren't\", \"can't\", \"could've\", \"couldn't\", \"didn't\", \"doesn't\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\", \"how's\", \"I'd\", \"I'd've\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\", \"it'll\", \"it's\", \"let's\", \"ma'am\", \"must've\", \"o'clock\", \"oughtn't\", \"she'd\", \"she'll\", \"she's\", \"should've\", \"shouldn't\", \"that's\", \"there's\", \"here's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"wasn't\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"weren't\", \"what're\", \"what's\", \"what've\", \"where'd\", \"where's\", \"who'll\", \"who's\", \"who've\", \"won't\", \"would've\", \"wouldn't\", \"wouldn't've\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"- Known Contractions -\")\n",
    "print(\"   Google News :\")\n",
    "print(known_contractions(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done copying to tr\n",
      "Done cleaning text\n",
      "Done cleaning numbers\n",
      "Done replacing mispells from between american and english language\n",
      "Done cleaning contractions\n",
      "Done cleaning special characters\n"
     ]
    }
   ],
   "source": [
    "tr = pd.DataFrame()\n",
    "tr['comment_text'] = train['comment_text']\n",
    "print('Done copying to tr')\n",
    "tr[\"comment_text\"] = tr[\"comment_text\"].apply(lambda x: clean_text(x))\n",
    "print('Done cleaning text')\n",
    "tr[\"comment_text\"] = tr[\"comment_text\"].apply(lambda x: clean_numbers(x))\n",
    "print('Done cleaning numbers')\n",
    "tr[\"comment_text\"] = tr[\"comment_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "print('Done replacing mispells from between american and english language')\n",
    "tr['comment_text'] = tr['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "print('Done cleaning contractions')\n",
    "tr['comment_text'] = tr['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "print('Done cleaning special characters')\n",
    "# sentences = tr[\"comment_text\"].apply(lambda x: x.split()).values\n",
    "# print('Done getting sentences')\n",
    "# vocab = build_vocab(sentences)\n",
    "# print('Done building the vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 37.284% of vocab\n",
      "Found embeddings for  99.246% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool It s like  would you want your...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you This would make my life a lot less a...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem kudos to...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I ll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text    target\n",
       "0  This is so cool It s like  would you want your...  0.000000\n",
       "1  Thank you This would make my life a lot less a...  0.000000\n",
       "2  This is such an urgent design problem kudos to...  0.000000\n",
       "3  Is this something I ll be able to install on m...  0.000000\n",
       "4                haha you guys are a bunch of losers  0.893617"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([tr,train[['target']]],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tr\n",
    "del train\n",
    "# del oov\n",
    "# del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = np.where(data['target'] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool It s like  would you want your...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you This would make my life a lot less a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  target\n",
       "0  This is so cool It s like  would you want your...   False\n",
       "1  Thank you This would make my life a lot less a...   False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534142 train comments, 270732 validate comments\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df = model_selection.train_test_split(data, test_size=0.15)\n",
    "print('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 100000\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df[TEXT_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 256\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIMENSION = 300\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,EMBEDDINGS_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_in_embedding = 0\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embeddings_index.vocab:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector        \n",
    "        num_words_in_embedding += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "train_labels = train_df[TOXICITY_COLUMN]\n",
    "validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "validate_labels = validate_df[TOXICITY_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 256, 300)     122910600   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 256, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 128)     140160      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 255, 64)      16448       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 123,067,337\n",
      "Trainable params: 156,737\n",
      "Non-trainable params: 122,910,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embed_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            EMBEDDINGS_DIMENSION,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "y = embed_layer(seq_input)\n",
    "y = SpatialDropout1D(0.2)(y)\n",
    "y = Bidirectional(CuDNNGRU(64, return_sequences=True))(y)   \n",
    "y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(y)\n",
    "max_pool = GlobalMaxPooling1D()(y)     \n",
    "\n",
    "y = concatenate([avg_pool, max_pool])\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "\n",
    "m = Model(seq_input, pred)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1534142 samples, validate on 270732 samples\n",
      "Epoch 1/100\n",
      " 378880/1534142 [======>.......................] - ETA: 3:03:36 - loss: 0.1914 - acc: 0.9349"
     ]
    }
   ],
   "source": [
    "m.fit(\n",
    "    train_text,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(validate_text, validate_labels),\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
